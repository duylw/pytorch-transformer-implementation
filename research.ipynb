{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c3f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer_model import Transformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2630edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Transformer(\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    max_seq_length\n",
    ").to(device)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length), device='cuda')  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length), device='cuda')  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68560484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.668381690979004\n",
      "Epoch: 2, Loss: 8.542909622192383\n",
      "Epoch: 3, Loss: 8.473676681518555\n",
      "Epoch: 4, Loss: 8.42013931274414\n",
      "Epoch: 5, Loss: 8.363524436950684\n",
      "Epoch: 6, Loss: 8.294819831848145\n",
      "Epoch: 7, Loss: 8.21558952331543\n",
      "Epoch: 8, Loss: 8.130414009094238\n",
      "Epoch: 9, Loss: 8.04964828491211\n",
      "Epoch: 10, Loss: 7.965593338012695\n",
      "Epoch: 11, Loss: 7.89173698425293\n",
      "Epoch: 12, Loss: 7.807148456573486\n",
      "Epoch: 13, Loss: 7.727425575256348\n",
      "Epoch: 14, Loss: 7.638842582702637\n",
      "Epoch: 15, Loss: 7.559998035430908\n",
      "Epoch: 16, Loss: 7.481446743011475\n",
      "Epoch: 17, Loss: 7.393308639526367\n",
      "Epoch: 18, Loss: 7.316126823425293\n",
      "Epoch: 19, Loss: 7.226165294647217\n",
      "Epoch: 20, Loss: 7.152009010314941\n",
      "Epoch: 21, Loss: 7.070465564727783\n",
      "Epoch: 22, Loss: 6.985903263092041\n",
      "Epoch: 23, Loss: 6.916940212249756\n",
      "Epoch: 24, Loss: 6.846054553985596\n",
      "Epoch: 25, Loss: 6.759749412536621\n",
      "Epoch: 26, Loss: 6.692696571350098\n",
      "Epoch: 27, Loss: 6.612946033477783\n",
      "Epoch: 28, Loss: 6.546875476837158\n",
      "Epoch: 29, Loss: 6.468862056732178\n",
      "Epoch: 30, Loss: 6.407159805297852\n",
      "Epoch: 31, Loss: 6.327588081359863\n",
      "Epoch: 32, Loss: 6.26312255859375\n",
      "Epoch: 33, Loss: 6.196388244628906\n",
      "Epoch: 34, Loss: 6.124568939208984\n",
      "Epoch: 35, Loss: 6.063038349151611\n",
      "Epoch: 36, Loss: 6.005049705505371\n",
      "Epoch: 37, Loss: 5.944539546966553\n",
      "Epoch: 38, Loss: 5.8736653327941895\n",
      "Epoch: 39, Loss: 5.8039774894714355\n",
      "Epoch: 40, Loss: 5.741833686828613\n",
      "Epoch: 41, Loss: 5.683141231536865\n",
      "Epoch: 42, Loss: 5.621458530426025\n",
      "Epoch: 43, Loss: 5.555394172668457\n",
      "Epoch: 44, Loss: 5.4980316162109375\n",
      "Epoch: 45, Loss: 5.435397148132324\n",
      "Epoch: 46, Loss: 5.380231857299805\n",
      "Epoch: 47, Loss: 5.317863464355469\n",
      "Epoch: 48, Loss: 5.267357349395752\n",
      "Epoch: 49, Loss: 5.202028274536133\n",
      "Epoch: 50, Loss: 5.149478912353516\n",
      "Epoch: 51, Loss: 5.0904364585876465\n",
      "Epoch: 52, Loss: 5.037724494934082\n",
      "Epoch: 53, Loss: 4.982343673706055\n",
      "Epoch: 54, Loss: 4.929906845092773\n",
      "Epoch: 55, Loss: 4.865334987640381\n",
      "Epoch: 56, Loss: 4.815459728240967\n",
      "Epoch: 57, Loss: 4.768134593963623\n",
      "Epoch: 58, Loss: 4.712333679199219\n",
      "Epoch: 59, Loss: 4.6521897315979\n",
      "Epoch: 60, Loss: 4.606597423553467\n",
      "Epoch: 61, Loss: 4.558772563934326\n",
      "Epoch: 62, Loss: 4.503574371337891\n",
      "Epoch: 63, Loss: 4.452182769775391\n",
      "Epoch: 64, Loss: 4.393276214599609\n",
      "Epoch: 65, Loss: 4.351943016052246\n",
      "Epoch: 66, Loss: 4.292489528656006\n",
      "Epoch: 67, Loss: 4.246134281158447\n",
      "Epoch: 68, Loss: 4.19420051574707\n",
      "Epoch: 69, Loss: 4.149869441986084\n",
      "Epoch: 70, Loss: 4.1010847091674805\n",
      "Epoch: 71, Loss: 4.053366184234619\n",
      "Epoch: 72, Loss: 4.002992153167725\n",
      "Epoch: 73, Loss: 3.9598634243011475\n",
      "Epoch: 74, Loss: 3.9127039909362793\n",
      "Epoch: 75, Loss: 3.8593761920928955\n",
      "Epoch: 76, Loss: 3.8180878162384033\n",
      "Epoch: 77, Loss: 3.7676167488098145\n",
      "Epoch: 78, Loss: 3.7232022285461426\n",
      "Epoch: 79, Loss: 3.67244291305542\n",
      "Epoch: 80, Loss: 3.626530408859253\n",
      "Epoch: 81, Loss: 3.5886547565460205\n",
      "Epoch: 82, Loss: 3.532970428466797\n",
      "Epoch: 83, Loss: 3.4879918098449707\n",
      "Epoch: 84, Loss: 3.435636520385742\n",
      "Epoch: 85, Loss: 3.3965578079223633\n",
      "Epoch: 86, Loss: 3.3467204570770264\n",
      "Epoch: 87, Loss: 3.3051273822784424\n",
      "Epoch: 88, Loss: 3.261798858642578\n",
      "Epoch: 89, Loss: 3.218501329421997\n",
      "Epoch: 90, Loss: 3.1712427139282227\n",
      "Epoch: 91, Loss: 3.127788543701172\n",
      "Epoch: 92, Loss: 3.080322504043579\n",
      "Epoch: 93, Loss: 3.0337226390838623\n",
      "Epoch: 94, Loss: 2.997593641281128\n",
      "Epoch: 95, Loss: 2.9496543407440186\n",
      "Epoch: 96, Loss: 2.907188892364502\n",
      "Epoch: 97, Loss: 2.872532606124878\n",
      "Epoch: 98, Loss: 2.8252556324005127\n",
      "Epoch: 99, Loss: 2.7837486267089844\n",
      "Epoch: 100, Loss: 2.740140676498413\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb618f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.8159761428833\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length), device=device)  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length), device=device)  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
